# Ralph Loop Progress - Legal Chatbot

## Learnings (Read This First)

### ScraperAPI Settings (CRITICAL)
- Gov.il requires `ultra_premium=true` - premium alone returns 500 errors
- Must use `wait_for=5000` for Angular to render
- Always use `render=true`
- Rate limit: 1 request per second

### Decision Title Patterns

**Decisive Appraiser (שמאי מכריע):**
```regex
/הכרעת שמאי מכריע מיום (\d{2}-\d{2}-\d{4}) בעניין ([^נ]+)נ ([^ג]+)ג (\d+) ח (\d+)\s*-?\s*(.+)?/
```

**Appeals Committee (ועדת השגות):**
```regex
/החלטה ב?השגה(?:\s+מס['׳]?\s*|\s+)(\d+)?\s*([^גג]+)?[גג](?:וש)?\s*(\d+)\s*[חח](?:לקה)?\s*(\d+)/
```

### Pinecone
- Host: https://gov-il-decisions-k1iqa9s.svc.aped-4627-b74a.pinecone.io
- Namespace: gov-il-decisions
- Dimension: 1024 (text-embedding-3-small)
- Max metadata size: 40KB per vector

### PDF URLs
- Format: `https://free-justice.openapi.gov.il/free/moj/portal/rest/searchpredefinedapi/v1/SearchPredefinedApi/Documents/DecisiveAppraiser/{docId}`
- NO .pdf extension needed

### Existing Components
- `workflows/chatbot-frontend.html` - Existing Hebrew UI with citations
- `workflows/3-rag-chatbot.json` - RAG workflow with hallucination detection
- `mcp-server/` - SQLite-based MCP server (separate from Pinecone)

### n8n Workflow IDs to Analyze
- oqihIkB7Ur9WVJZG
- kTZqcClvtUspeC28
- McOa9j15PRy8AZ8v

---

## Iteration Log

## Iteration 1 — Map Existing n8n Workflows
- **Date**: 2026-01-21
- **Task**: US-P1-001 - Map existing n8n workflows

### Workflow Analysis Results

#### Workflow 1: `oqihIkB7Ur9WVJZG` - "Gov.il Decisive Appraisal Scraper - HTML ALL PAGES"
- **Status**: Active
- **Purpose**: Scrapes decisive appraisal decisions from gov.il
- **Trigger**: Webhook (POST `/run-scraper`) + Manual
- **Flow**:
  1. Set Config (ScraperAPI key, currentSkip=0)
  2. Fetch page via ScraperAPI (uses `premium=true`, NOT `ultra_premium`)
  3. Extract Documents from HTML (regex for titles, PDF URLs, dates)
  4. Merge with Config (accumulates documents)
  5. Has More Pages? → Loop back or Send to Processor
  6. Send to Document Processor (webhook call)
- **Stats**: 10 executions, 90% success rate
- **ISSUE**: Uses `premium=true` instead of required `ultra_premium=true`
- **Nodes**: 10 nodes total

#### Workflow 2: `kTZqcClvtUspeC28` - "Document Processor - Embeddings & Pinecone"
- **Status**: Active
- **Purpose**: Processes documents, creates embeddings, upserts to Pinecone
- **Trigger**: Webhook (POST `/process-documents`)
- **Flow**:
  1. Webhook Trigger → Prepare Documents
  2. Fetch PDF (direct URL, handles special char encoding)
  3. Extract PDF Text (uses extractFromFile node)
  4. Split Into Pages (~2000 char chunks)
  5. Create Embedding (OpenAI text-embedding-3-small, 1024 dimensions)
  6. Combine Results → Upsert to Pinecone
  7. Aggregate Results → Return Result → Respond
- **Stats**: 10 executions, 100% success rate
- **Pinecone**: Uses namespace `gov-il-decisions`, metadata includes title, url, committee, block, plot, appraiser
- **Nodes**: 11 nodes total

#### Workflow 3: `McOa9j15PRy8AZ8v` - "RAG Chatbot - Decisive Appraisal (with Hallucination Detection)"
- **Status**: Active
- **Purpose**: RAG chatbot with Strawberry/Pythea hallucination detection
- **Trigger**: Webhook (POST `/chat`)
- **Flow**:
  1. Chat Webhook → Extract User Message
  2. Embed User Query (OpenAI text-embedding-3-small)
  3. Query Pinecone (topK=5)
  4. Build RAG Context (includes citation instructions [S0], [S1])
  5. Generate AI Response (GPT-4o)
  6. Extract Claims & Citations
  7. Build Verification Prompts
  8. Verify Posterior (Full Context) + Verify Prior (Scrubbed Context) [PARALLEL]
  9. Merge Verification Results
  10. Compute Budget Gaps (KL Divergence)
  11. Format Response with Flags → Respond
- **Stats**: 10 executions, 50% success rate (needs investigation)
- **Hallucination Detection**: Implements Strawberry/Pythea KL-divergence algorithm
- **Nodes**: 14 nodes total

### Key Findings
1. **Scraper uses wrong ScraperAPI setting**: `premium=true` should be `ultra_premium=true`
2. **Document Processor works well**: 100% success rate
3. **RAG Chatbot has issues**: 50% success rate needs debugging
4. **PDF chunking**: Splits into ~2000 char chunks (not whole doc vectors as PRD specifies)
5. **Citation format**: Uses [S0], [S1], etc. as intended

### Learnings for Next Cycles
- ScraperAPI settings are critical - always verify `ultra_premium=true`
- The existing hallucination detection flow is complete but may need tuning
- Document chunking approach differs from PRD "one doc = one vector" requirement

---

## Iteration 2 — Document Current Pinecone Schema and Data
- **Date**: 2026-01-21
- **Task**: US-P1-001 - Document current Pinecone schema and data

### Pinecone Configuration

| Setting | Value |
|---------|-------|
| Host | https://gov-il-decisions-k1iqa9s.svc.aped-4627-b74a.pinecone.io |
| Namespace | gov-il-decisions |
| Dimension | 1024 (text-embedding-3-small) |
| Max metadata size | 40KB per vector |

### Current Pinecone Vector Schema (from n8n Document Processor)

The "Combine Results" node in workflow `kTZqcClvtUspeC28` defines the vector schema:

```typescript
interface PineconeVector {
  id: string;           // Format: {documentId}_p{pageNumber} e.g., "doc_1705123456789_0_p1"
  values: number[];     // 1024-dimension embedding array
  metadata: {
    // Document identification
    documentId: string;        // Generated ID: "doc_{timestamp}_{index}"
    title: string;             // Full Hebrew title of the decision
    url: string;               // PDF URL (free-justice.openapi.gov.il)

    // Structured fields (for filtering)
    committee: string;         // ועדה מקומית (local committee name)
    block: string;             // גוש (block number)
    plot: string;              // חלקה (plot number)
    appraiser: string;         // שמאי name (for decisive_appraiser only)
    publishDate: string;       // Publication date

    // Page-specific fields
    pageNumber: number;        // Page/chunk number (1-indexed)
    totalPages: number;        // Total pages/chunks in document
    content: string;           // Text content (truncated to 10,000 chars)
    contentLength: number;     // Original content length
  }
}
```

### Current Schema vs PRD Schema Gap Analysis

| Field | Current Schema | PRD Schema | Gap |
|-------|---------------|------------|-----|
| id | `{docId}_p{page}` | `{database}-{hash12}` | ❌ Different format, current is page-based |
| database | ❌ Missing | Required | ❌ No database field (always decisive_appraiser implied) |
| title | ✅ Present | Required | ✅ |
| url | ✅ Present | Required | ✅ |
| block | ✅ Present | Required | ✅ |
| plot | ✅ Present | Required | ✅ |
| committee | ✅ Present | Required | ✅ |
| appraiser | ✅ Present | Required | ✅ |
| caseType | ❌ Missing | Required | ❌ Not extracted |
| decisionDate | ❌ Missing | Required | ❌ Not extracted (only publishDate) |
| year | ❌ Missing | Required | ❌ Not extracted for filtering |
| description | content field | Required | ⚠️ Limited to 10KB, not full PDF text |
| contentHash | ❌ Missing | Required | ❌ No deduplication |
| indexedAt | ❌ Missing | Required | ❌ No timestamp |

### Key Issues Identified

1. **Document Chunking**: Current system splits PDFs into ~2000 char chunks, creating multiple vectors per document
   - PRD requires: "One document = one vector" (no splitting)
   - This affects citation accuracy and statistics queries

2. **Missing Fields**:
   - `database` - Cannot distinguish between 3 sources
   - `caseType` - Cannot filter by היטל השבחה, פיצויים, etc.
   - `decisionDate` - Cannot do date-based filtering/statistics
   - `year` - Cannot do year-based analytics queries
   - `contentHash` - No deduplication mechanism

3. **Metadata Fields Expected by RAG Chatbot**:
   The RAG workflow queries for: `meta.title`, `meta.description`, `meta.text`, `meta.content`, `meta.url`, `meta.publishDate`
   - Note: Uses fallback chain: `description || text || content`

### Data Coverage (Estimated)

Based on workflow analysis:
- Only `decisive_appraiser` database is being indexed
- `appeals_committee` and `appeals_board` not yet connected
- Total vectors unknown without direct Pinecone stats query (requires API call)

### Learnings for Next Cycles
- Pinecone schema needs significant updates to match PRD requirements
- Must add database field to distinguish between 3 sources
- Need to switch from chunked approach to whole-document vectors
- caseType and decisionDate extraction already exists in MCP server scraper - can reuse
- contentHash is generated in scraper's `toDecision()` method - can reuse

---
