# Ralph Loop Progress - Legal Chatbot

## Learnings (Read This First)

### ScraperAPI Settings (CRITICAL)
- Gov.il requires `ultra_premium=true` - premium alone returns 500 errors
- Must use `wait_for=5000` for Angular to render
- Always use `render=true`
- Rate limit: 1 request per second

### Decision Title Patterns

**Decisive Appraiser (שמאי מכריע):**
```regex
/הכרעת שמאי מכריע מיום (\d{2}-\d{2}-\d{4}) בעניין ([^נ]+)נ ([^ג]+)ג (\d+) ח (\d+)\s*-?\s*(.+)?/
```

**Appeals Committee (ועדת השגות):**
```regex
/החלטה ב?השגה(?:\s+מס['׳]?\s*|\s+)(\d+)?\s*([^גג]+)?[גג](?:וש)?\s*(\d+)\s*[חח](?:לקה)?\s*(\d+)/
```

### Pinecone
- Host: https://gov-il-decisions-k1iqa9s.svc.aped-4627-b74a.pinecone.io
- Namespace: gov-il-decisions
- Dimension: 1024 (text-embedding-3-small)
- Max metadata size: 40KB per vector

### PDF URLs
- Format: `https://free-justice.openapi.gov.il/free/moj/portal/rest/searchpredefinedapi/v1/SearchPredefinedApi/Documents/DecisiveAppraiser/{docId}`
- NO .pdf extension needed

### Existing Components
- `workflows/chatbot-frontend.html` - Existing Hebrew UI with citations
- `workflows/3-rag-chatbot.json` - RAG workflow with hallucination detection
- `mcp-server/` - SQLite-based MCP server (separate from Pinecone)

### n8n Workflow IDs to Analyze
- oqihIkB7Ur9WVJZG
- kTZqcClvtUspeC28
- McOa9j15PRy8AZ8v

---

## Iteration Log

## Iteration 1 — Map Existing n8n Workflows
- **Date**: 2026-01-21
- **Task**: US-P1-001 - Map existing n8n workflows

### Workflow Analysis Results

#### Workflow 1: `oqihIkB7Ur9WVJZG` - "Gov.il Decisive Appraisal Scraper - HTML ALL PAGES"
- **Status**: Active
- **Purpose**: Scrapes decisive appraisal decisions from gov.il
- **Trigger**: Webhook (POST `/run-scraper`) + Manual
- **Flow**:
  1. Set Config (ScraperAPI key, currentSkip=0)
  2. Fetch page via ScraperAPI (uses `premium=true`, NOT `ultra_premium`)
  3. Extract Documents from HTML (regex for titles, PDF URLs, dates)
  4. Merge with Config (accumulates documents)
  5. Has More Pages? → Loop back or Send to Processor
  6. Send to Document Processor (webhook call)
- **Stats**: 10 executions, 90% success rate
- **ISSUE**: Uses `premium=true` instead of required `ultra_premium=true`
- **Nodes**: 10 nodes total

#### Workflow 2: `kTZqcClvtUspeC28` - "Document Processor - Embeddings & Pinecone"
- **Status**: Active
- **Purpose**: Processes documents, creates embeddings, upserts to Pinecone
- **Trigger**: Webhook (POST `/process-documents`)
- **Flow**:
  1. Webhook Trigger → Prepare Documents
  2. Fetch PDF (direct URL, handles special char encoding)
  3. Extract PDF Text (uses extractFromFile node)
  4. Split Into Pages (~2000 char chunks)
  5. Create Embedding (OpenAI text-embedding-3-small, 1024 dimensions)
  6. Combine Results → Upsert to Pinecone
  7. Aggregate Results → Return Result → Respond
- **Stats**: 10 executions, 100% success rate
- **Pinecone**: Uses namespace `gov-il-decisions`, metadata includes title, url, committee, block, plot, appraiser
- **Nodes**: 11 nodes total

#### Workflow 3: `McOa9j15PRy8AZ8v` - "RAG Chatbot - Decisive Appraisal (with Hallucination Detection)"
- **Status**: Active
- **Purpose**: RAG chatbot with Strawberry/Pythea hallucination detection
- **Trigger**: Webhook (POST `/chat`)
- **Flow**:
  1. Chat Webhook → Extract User Message
  2. Embed User Query (OpenAI text-embedding-3-small)
  3. Query Pinecone (topK=5)
  4. Build RAG Context (includes citation instructions [S0], [S1])
  5. Generate AI Response (GPT-4o)
  6. Extract Claims & Citations
  7. Build Verification Prompts
  8. Verify Posterior (Full Context) + Verify Prior (Scrubbed Context) [PARALLEL]
  9. Merge Verification Results
  10. Compute Budget Gaps (KL Divergence)
  11. Format Response with Flags → Respond
- **Stats**: 10 executions, 50% success rate (needs investigation)
- **Hallucination Detection**: Implements Strawberry/Pythea KL-divergence algorithm
- **Nodes**: 14 nodes total

### Key Findings
1. **Scraper uses wrong ScraperAPI setting**: `premium=true` should be `ultra_premium=true`
2. **Document Processor works well**: 100% success rate
3. **RAG Chatbot has issues**: 50% success rate needs debugging
4. **PDF chunking**: Splits into ~2000 char chunks (not whole doc vectors as PRD specifies)
5. **Citation format**: Uses [S0], [S1], etc. as intended

### Learnings for Next Cycles
- ScraperAPI settings are critical - always verify `ultra_premium=true`
- The existing hallucination detection flow is complete but may need tuning
- Document chunking approach differs from PRD "one doc = one vector" requirement

---

## Iteration 2 — Document Current Pinecone Schema and Data
- **Date**: 2026-01-21
- **Task**: US-P1-001 - Document current Pinecone schema and data

### Pinecone Configuration

| Setting | Value |
|---------|-------|
| Host | https://gov-il-decisions-k1iqa9s.svc.aped-4627-b74a.pinecone.io |
| Namespace | gov-il-decisions |
| Dimension | 1024 (text-embedding-3-small) |
| Max metadata size | 40KB per vector |

### Current Pinecone Vector Schema (from n8n Document Processor)

The "Combine Results" node in workflow `kTZqcClvtUspeC28` defines the vector schema:

```typescript
interface PineconeVector {
  id: string;           // Format: {documentId}_p{pageNumber} e.g., "doc_1705123456789_0_p1"
  values: number[];     // 1024-dimension embedding array
  metadata: {
    // Document identification
    documentId: string;        // Generated ID: "doc_{timestamp}_{index}"
    title: string;             // Full Hebrew title of the decision
    url: string;               // PDF URL (free-justice.openapi.gov.il)

    // Structured fields (for filtering)
    committee: string;         // ועדה מקומית (local committee name)
    block: string;             // גוש (block number)
    plot: string;              // חלקה (plot number)
    appraiser: string;         // שמאי name (for decisive_appraiser only)
    publishDate: string;       // Publication date

    // Page-specific fields
    pageNumber: number;        // Page/chunk number (1-indexed)
    totalPages: number;        // Total pages/chunks in document
    content: string;           // Text content (truncated to 10,000 chars)
    contentLength: number;     // Original content length
  }
}
```

### Current Schema vs PRD Schema Gap Analysis

| Field | Current Schema | PRD Schema | Gap |
|-------|---------------|------------|-----|
| id | `{docId}_p{page}` | `{database}-{hash12}` | ❌ Different format, current is page-based |
| database | ❌ Missing | Required | ❌ No database field (always decisive_appraiser implied) |
| title | ✅ Present | Required | ✅ |
| url | ✅ Present | Required | ✅ |
| block | ✅ Present | Required | ✅ |
| plot | ✅ Present | Required | ✅ |
| committee | ✅ Present | Required | ✅ |
| appraiser | ✅ Present | Required | ✅ |
| caseType | ❌ Missing | Required | ❌ Not extracted |
| decisionDate | ❌ Missing | Required | ❌ Not extracted (only publishDate) |
| year | ❌ Missing | Required | ❌ Not extracted for filtering |
| description | content field | Required | ⚠️ Limited to 10KB, not full PDF text |
| contentHash | ❌ Missing | Required | ❌ No deduplication |
| indexedAt | ❌ Missing | Required | ❌ No timestamp |

### Key Issues Identified

1. **Document Chunking**: Current system splits PDFs into ~2000 char chunks, creating multiple vectors per document
   - PRD requires: "One document = one vector" (no splitting)
   - This affects citation accuracy and statistics queries

2. **Missing Fields**:
   - `database` - Cannot distinguish between 3 sources
   - `caseType` - Cannot filter by היטל השבחה, פיצויים, etc.
   - `decisionDate` - Cannot do date-based filtering/statistics
   - `year` - Cannot do year-based analytics queries
   - `contentHash` - No deduplication mechanism

3. **Metadata Fields Expected by RAG Chatbot**:
   The RAG workflow queries for: `meta.title`, `meta.description`, `meta.text`, `meta.content`, `meta.url`, `meta.publishDate`
   - Note: Uses fallback chain: `description || text || content`

### Data Coverage (Estimated)

Based on workflow analysis:
- Only `decisive_appraiser` database is being indexed
- `appeals_committee` and `appeals_board` not yet connected
- Total vectors unknown without direct Pinecone stats query (requires API call)

### Learnings for Next Cycles
- Pinecone schema needs significant updates to match PRD requirements
- Must add database field to distinguish between 3 sources
- Need to switch from chunked approach to whole-document vectors
- caseType and decisionDate extraction already exists in MCP server scraper - can reuse
- contentHash is generated in scraper's `toDecision()` method - can reuse

---

## Iteration 3 — Identify What Documents Are Already Indexed
- **Date**: 2026-01-21
- **Task**: US-P1-001 - Identify what documents are already indexed

### Pinecone Index Stats (Direct API Query)

| Metric | Value |
|--------|-------|
| Namespace | gov-il-decisions |
| Total Vector Count | **73 vectors** |
| Dimension | 1024 |
| Index Fullness | 0% |

### Critical Finding: Very Low Coverage

The Pinecone index contains only **73 vectors** in the `gov-il-decisions` namespace.

Based on PRD targets:
- **Target**: ~20,000+ decisions across 3 databases
- **Current**: 73 vectors
- **Coverage**: ~0.4% (73 / 20,000)

### Vector Distribution Analysis

From the Document Processor workflow (kTZqcClvtUspeC28):
- Documents are split into ~2000 char chunks (multiple vectors per document)
- This means actual unique documents indexed is likely ~10-20 (assuming 3-7 vectors per document)
- Only `decisive_appraiser` database is being indexed

### Databases Coverage

| Database | Hebrew Name | Estimated Docs | Indexed | Coverage |
|----------|-------------|----------------|---------|----------|
| decisive_appraiser | שמאי מכריע | ~10,000 | ~10-20 | <0.2% |
| appeals_committee | ועדת השגות | ~5,000 | 0 | 0% |
| appeals_board | ועדת ערעורים | ~5,000 | 0 | 0% |

### Root Causes

1. **Scraper workflow issue**: Uses `premium=true` instead of required `ultra_premium=true`
2. **Limited execution**: Only 10 executions of scraper workflow with 90% success rate
3. **No appeals data**: Only decisive_appraiser is connected
4. **Chunking reduces unique docs**: Multiple vectors per document reduces actual coverage

### Learnings for Next Cycles
- Pinecone has barely any data - full reindex required
- Fix ScraperAPI settings before running full index
- Must connect appeals_committee and appeals_board databases
- PRD requirement "one doc = one vector" not yet implemented

---

## Iteration 5 — Document Current Hallucination Detection Flow
- **Date**: 2026-01-21
- **Task**: US-P1-001 - Document current hallucination detection flow

### Workflow Overview

The hallucination detection is implemented in workflow `McOa9j15PRy8AZ8v` ("RAG Chatbot - Decisive Appraisal with Hallucination Detection"). It uses the **Strawberry/Pythea KL-divergence algorithm** to detect when AI claims are not grounded in retrieved evidence.

### Architecture Diagram

```
User Query → Embed Query → Pinecone (topK=5) → Build RAG Context → GPT-4o Response
                                                                        ↓
                                                    Extract Claims & Citations
                                                                        ↓
                                                    Build Verification Prompts
                                                          ↓           ↓
                                           ┌──────────────┴───────────┴──────────────┐
                                           │                                          │
                                   Verify Posterior              Verify Prior
                                   (Full Context)                (Scrubbed Context)
                                   gpt-4o-mini + logprobs        gpt-4o-mini + logprobs
                                           │                                          │
                                           └──────────────┬───────────┬──────────────┘
                                                          ↓           ↓
                                                    Merge Results (combineByPosition)
                                                                        ↓
                                                    Compute Budget Gaps (KL Divergence)
                                                                        ↓
                                                    Format Response with Flags
                                                                        ↓
                                                    Respond with Chat
```

### Node-by-Node Analysis (14 nodes total)

#### 1. Chat Webhook (chat-webhook)
- **Type**: webhook (POST `/chat`)
- **Purpose**: Entry point for chat requests
- **Response Mode**: responseNode (async)

#### 2. Extract User Message (extract-message)
- **Purpose**: Parse user message and conversation history
- **Input**: `body.message` or `body.query`
- **Output**: `{ userMessage, conversationHistory, timestamp }`

#### 3. Embed User Query (embed-query)
- **Model**: text-embedding-3-small (1024 dimensions)
- **API**: OpenAI /v1/embeddings

#### 4. Query Pinecone (query-pinecone)
- **topK**: 5 documents
- **Namespace**: gov-il-decisions
- **includeMetadata**: true

#### 5. Build RAG Context (build-context)
- **Purpose**: Prepare context for AI and store raw docs for verification
- **Key Features**:
  - Assigns source IDs: `S0`, `S1`, `S2`...
  - Stores `rawDocuments` array for hallucination checking
  - Builds Hebrew system prompt with citation instructions
  - Instructs AI to cite sources as `[S0]`, `[S1]`, etc.

#### 6. Generate AI Response (generate-response)
- **Model**: GPT-4o
- **Temperature**: 0.7
- **Max Tokens**: 1000

#### 7. Extract Claims & Citations (extract-claims)
- **Purpose**: Split AI response into claims and identify citations
- **Sentence Pattern**: `/[^.!?\\n]+[.!?]?/g` (Hebrew-aware)
- **Citation Pattern**: `/\\[S(\\d+)\\]/g`
- **Filters**: Claims must be >10 characters
- **Output per claim**:
  ```json
  {
    "id": "claim-0",
    "originalText": "לפי ההחלטה [S0], השמאי קבע פיצוי.",
    "cleanClaim": "לפי ההחלטה, השמאי קבע פיצוי.",
    "citations": ["S0"]
  }
  ```

#### 8. Build Verification Prompts (build-verification-prompts)
- **Purpose**: Create two contexts for each claim
- **Full Context**: All retrieved documents
- **Scrubbed Context**: Cited documents replaced with `[REDACTED]`
- **Limits**: Max 10 claims, min 15 chars per claim
- **Output format**:
  ```json
  {
    "claimId": "claim-0",
    "claim": "השמאי קבע פיצוי",
    "citations": ["S0"],
    "hasCitations": true,
    "fullContext": "[S0]: text... [S1]: text...",
    "scrubbedContext": "[S0]: [REDACTED] [S1]: text..."
  }
  ```

#### 9. Verify Posterior - Full Context (verify-posterior)
- **Model**: gpt-4o-mini
- **Purpose**: Ask "Is this claim entailed by the full context?"
- **Settings**: `logprobs: true, top_logprobs: 5, max_tokens: 5`
- **Response**: YES/NO/UNSURE with probability scores
- **Batching**: 10 claims, 100ms interval

#### 10. Verify Prior - Scrubbed Context (verify-prior)
- **Model**: gpt-4o-mini
- **Purpose**: Ask "Is this claim entailed by the scrubbed context?"
- **Key Insight**: If cited sources are `[REDACTED]`, the verifier cannot see the evidence. If it still says YES, the claim is likely a hallucination.
- **Settings**: Same as posterior

#### 11. Merge Verification Results (merge-verification)
- **Type**: Merge node (combineByPosition)
- **Purpose**: Pair posterior and prior results for each claim

#### 12. Compute Budget Gaps - KL Divergence (compute-budget-gaps)

**Core Strawberry/Pythea Algorithm:**

```javascript
// Extract probability of "YES" from logprobs
function extractPYes(response) {
  const logprobs = response?.choices?.[0]?.logprobs;
  if (!logprobs) {
    // Fallback: text-based detection
    const text = response?.choices?.[0]?.message?.content?.toUpperCase();
    if (text.includes('YES')) return 0.85;
    if (text.includes('NO')) return 0.15;
    return 0.5;
  }
  // Extract from top_logprobs
  const yesEntry = logprobs.content[0].top_logprobs.find(t =>
    t.token.toUpperCase().trim() === 'YES'
  );
  return yesEntry ? Math.exp(yesEntry.logprob) : 0.5;
}

// KL Divergence for Bernoulli distributions
function klBernoulli(p, q) {
  const eps = 1e-12;
  p = Math.max(eps, Math.min(1 - eps, p));
  q = Math.max(eps, Math.min(1 - eps, q));
  return p * Math.log(p / q) + (1 - p) * Math.log((1 - p) / (1 - q));
}

// Grounding score computation
function computeGroundingScore(p1, p0, hasCitations) {
  // p1 = P(entailed | full context)
  // p0 = P(entailed | scrubbed context)

  const observed = klBernoulli(p1, 0.5);   // How much model believes the claim
  const required = klBernoulli(p1, p0);    // How much the evidence changed belief
  const budgetGap = observed - required;

  const evidenceUse = Math.max(0, p1 - p0);  // Did evidence increase confidence?
  const evidenceUsed = hasCitations ? evidenceUse > 0.15 : true;

  // Confidence calculation
  let confidence;
  if (!hasCitations) {
    confidence = p1 > 0.7 ? p1 * 0.7 : p1 * 0.4;  // Uncited claims penalized
  } else {
    confidence = Math.min(1, evidenceUse * 1.5 + (p1 > 0.7 ? 0.3 : 0));
  }

  const isGrounded = confidence > 0.45 && evidenceUsed;

  return { confidence, grounded: isGrounded, evidence_use: evidenceUse };
}
```

**Key Insight**: If `p1 ≈ p0`, the model's confidence didn't change when evidence was hidden → the claim is NOT grounded in evidence → FLAG as hallucination.

#### 13. Format Response with Flags (format-response)
- **Purpose**: Build final API response with hallucination data
- **Overall Grounding Threshold**: 70% of claims must be grounded
- **Warning Generation**: Hebrew warning for ungrounded claims

#### 14. Respond with Chat (respond-chat)
- **Headers**: CORS enabled (`Access-Control-Allow-Origin: *`)
- **Response**: Full JSON with hallucination_check

### API Response Schema

```json
{
  "success": true,
  "response": "לפי ההחלטה [S0], השמאי קבע...",
  "sources": [
    {"title": "...", "url": "...", "score": 0.92}
  ],
  "matchCount": 5,
  "model": "gpt-4o",
  "usage": {"prompt_tokens": 1234, "completion_tokens": 567},
  "hallucination_check": {
    "overall_grounded": true,
    "grounded_claims": 3,
    "total_claims": 4,
    "grounding_ratio": 0.75,
    "claims": [
      {
        "text": "השמאי קבע פיצוי של 100,000 ש\"ח",
        "citing": ["S0"],
        "p1": 0.92,
        "p0": 0.23,
        "observed_bits": 0.531,
        "required_bits": 1.234,
        "budget_gap": -0.703,
        "evidence_use": 0.69,
        "confidence": 0.88,
        "evidence_used": true,
        "grounded": true,
        "warning": null
      },
      {
        "text": "זה נפוץ במקרים דומים",
        "citing": [],
        "p1": 0.65,
        "p0": 0.61,
        "evidence_use": 0.04,
        "confidence": 0.26,
        "grounded": false,
        "warning": "לא נמצא מקור מספק"
      }
    ]
  },
  "warning": "שים לב: 1 טענות בתשובה לא נתמכות במלואן על ידי המקורות."
}
```

### Configuration Thresholds

| Parameter | Value | Purpose |
|-----------|-------|---------|
| Evidence use threshold | 0.15 | Minimum p1-p0 difference for cited claims |
| Confidence threshold | 0.45 | Minimum confidence to be grounded |
| Overall grounding | 70% | % claims that must be grounded |
| Max claims verified | 10 | Performance limit |
| Min claim length | 15 chars | Skip trivial sentences |
| Verifier model | gpt-4o-mini | Fast, cheap verification |

### Known Issues (from 50% success rate)

1. **Merge node pairing**: The Merge node combines results by position, but async HTTP batching may cause misalignment
2. **Logprobs extraction**: Fallback to text-based detection may be inaccurate
3. **Hebrew tokenization**: Sentence splitting may not handle Hebrew punctuation correctly
4. **Empty claims**: If no claims extracted, verification loop skipped

### Learnings for Next Cycles

- The Strawberry/Pythea algorithm is fully implemented and theoretically sound
- The 50% success rate suggests issues with data flow, not algorithm
- The frontend expects this exact response format - any changes must maintain compatibility
- Confidence thresholds may need tuning based on real-world results
- Consider adding error handling for API failures in verification nodes

---

## Iteration 4 — List Current Frontend Features
- **Date**: 2026-01-21
- **Task**: US-P1-001 - List current frontend features

### File Analyzed
- `workflows/chatbot-frontend.html` (1097 lines)

### Current Frontend Features

#### 1. Layout & Design
| Feature | Status | Notes |
|---------|--------|-------|
| Hebrew RTL layout | ✅ Implemented | `dir="rtl"` on html element |
| Dark header | ✅ Implemented | Gradient #1a1a2e → #16213e |
| Title "צ'אטבוט שמאות מכריעה" | ✅ Implemented | Header h1 |
| Clean chat bubbles | ✅ Implemented | User (right, purple) / Assistant (left, white) |
| Typing indicator | ✅ Implemented | 3-dot animated loader |
| Error handling | ✅ Implemented | Hebrew error messages with auto-dismiss |
| Mobile responsive | ✅ Implemented | @media query for ≤600px |

#### 2. Interactive Citation System
| Feature | Status | Notes |
|---------|--------|-------|
| [S#] tags converted to badges | ✅ Implemented | Blue badges with numbers |
| Hover → tooltip | ✅ Implemented | Shows title, relevance bar, actions |
| Click → highlights source | ✅ Implemented | Yellow glow animation + scroll |
| Open PDF button | ✅ Implemented | In tooltip and source list |
| Multiple citation grouping | ⚠️ Partial | Consecutive [S0][S1] render separately |

#### 3. Sources Section
| Feature | Status | Notes |
|---------|--------|-------|
| Collapsible section | ✅ Implemented | Toggle with ▼ icon |
| Numbered badges | ✅ Implemented | 1-indexed, matches inline citations |
| Color-coded relevance | ✅ Implemented | Green (≥80%), Yellow (50-79%), Red (<50%) |
| Click → source highlight | ✅ Implemented | 2-second yellow glow animation |
| Clickable PDF links | ✅ Implemented | Opens in new tab |

#### 4. Grounding Indicator (Hallucination Detection UI)
| Feature | Status | Notes |
|---------|--------|-------|
| Overall grounding badge | ✅ Implemented | Shows ✓/⚠/✗ with percentage |
| Green badge (≥90%) | ✅ "מבוסס" | |
| Yellow badge (70-89%) | ✅ "מבוסס חלקית" | |
| Red badge (<70%) | ✅ "דורש בדיקה" | |
| Expandable per-claim breakdown | ✅ Implemented | Click badge to expand |
| Per-claim confidence % | ✅ Implemented | Shows ✓/✗ icon + percentage |
| Warning banner | ✅ Implemented | Orange banner for ungrounded claims |

#### 5. Configuration
| Feature | Status | Notes |
|---------|--------|-------|
| Webhook URL input | ✅ Implemented | User configures n8n endpoint |
| Conversation history | ✅ Implemented | Maintains last 10 messages |

#### 6. UX Details
| Feature | Status | Notes |
|---------|--------|-------|
| Send on Enter | ✅ Implemented | |
| Auto-focus input | ✅ Implemented | On page load |
| Button disabled during send | ✅ Implemented | Prevents double-submit |
| Auto-scroll to bottom | ✅ Implemented | On new messages |
| XSS protection | ✅ Implemented | escapeHtml() function |

### Gap Analysis vs PRD Requirements

| PRD Requirement | Current State |
|-----------------|---------------|
| US-P4-001: Professional Hebrew UI | ✅ Complete |
| US-P4-002: Source display panel | ✅ Complete |
| US-P4-003: Interactive citations | ✅ Complete |
| US-P4-004: Analytics dashboard/charts | ❌ Not implemented |

### Expected API Response Format (Frontend Expects)
```json
{
  "success": true,
  "response": "לפי ההחלטה [S0], השמאי קבע...",
  "sources": [
    {"title": "...", "url": "...", "score": 0.92}
  ],
  "hallucination_check": {
    "grounding_ratio": 0.75,
    "total_claims": 4,
    "grounded_claims": 3,
    "claims": [
      {"text": "...", "grounded": true, "confidence": 0.88}
    ]
  },
  "warning": "שים לב: 1 טענות לא נתמכות"
}
```

### Learnings for Next Cycles
- Frontend is feature-complete for citation display and hallucination UI
- Phase 4 frontend tasks (US-P4-001 to US-P4-003) already largely implemented
- Only US-P4-004 (Analytics Dashboard with charts) is missing
- The frontend expects a specific API response format - RAG workflow must match
- Tooltip positioning has mobile-specific behavior (bottom sheet vs floating)

---
